import { exec } from 'child_process';
import { Command } from 'commander';
import { rmSync } from 'fs';
import * as _ from 'lodash';
import {
  isEmpty,
  startsWith,
} from 'lodash';
import { join } from 'path';
import * as util from 'util';

import {
  prefixAllLines,
  trimAllLines,
} from '@lukebechtel/lab-ts-utils';

import {
  dumpFunctionBySchemaAndNameCommandString,
  dumpTableBySchemaAndNameCommandString,
  dumpViewBySchemaAndNameCommandString,
  getFunctionNamesFromPostgres,
  getTableDescriptionCommandString,
  getTableNamesFromPostgres,
  getViewNamesFromPostgres,
  pgDumpDb,
  runMultipleDatabaseCommandsObj,
} from '../../dbUtils';
import { logTime } from '../../timingUtils';
import {
  isWhitespace,
  notEmpty,
  reasonote_ROOT_DIR_PATH,
  writeFileRecursiveCreate,
} from '../../utils';

const SUPABASE_ROOT_DIR_PATH = join(reasonote_ROOT_DIR_PATH, "supabase");
const SUPABASE_SCHEMA_DIR_PATH = join(SUPABASE_ROOT_DIR_PATH, "schema");

function prefixLines(instr: string, prefix: string) {
  return prefix + instr.slice(0).replaceAll("\n", `\n${prefix}`);
}

function sectionWrapContents(sectionName: string, contents: string) {
  const str = `
    ${_.repeat("-", LONG_LINE_LENGTH)}
    -- BEGIN: ${sectionName}
    ${_.repeat("-", SHORT_LINE_LENGTH)}
    _______SECRET_SECTION_WRAP_CONTENTS_______
    ${_.repeat("-", SHORT_LINE_LENGTH)}
    -- END: ${sectionName}
    ${_.repeat("-", LONG_LINE_LENGTH)} 
    `
    .split("\n")
    .map(_.trim)
    .join("\n");

  return str.replace("_______SECRET_SECTION_WRAP_CONTENTS_______", contents);
}

const LONG_LINE_LENGTH = 75;
const SHORT_LINE_LENGTH = 30;
const WARN_AUTOGEN_MSG = `
⚠️ DO NOT EDIT THIS FILE!
THIS FILE WAS AUTOMATICALLY GENERATED FROM THE CONTENTS OF A CLEANLY-MIGRATED DATABASE.

ℹ️ To change the contents of this file, create a new migration under 'supabase/migrations/'
that makes the change you would like to see.
`;
const WARN_AUTOGEN_SQL = `${_.repeat("-", LONG_LINE_LENGTH)}\n${prefixLines(
  WARN_AUTOGEN_MSG,
  "-- "
)}\n${_.repeat("-", LONG_LINE_LENGTH)}`;

interface GetRegenSchemasReturn {
  [schemaName: string]: {
    meta: {
      numTables: number;
      numFunctions: number;
      numViews: number;
    };
    entities: {
      schemaName: string;
      entityName: string;
      entityType: "table" | "function" | "view";
      dataToWrite: string;
    }[];
  };
}

async function getRegenSchemas(
  schemaNames: string[]
): Promise<GetRegenSchemasReturn> {
  // Path_To_Write: Contents_To_Write
  const RET: GetRegenSchemasReturn = Object.fromEntries(
    schemaNames.map((sn) => [
      sn,
      {
        meta: { numTables: 0, numFunctions: 0, numViews: 0 },
        entities: [],
      },
    ])
  );

  // First, we ask postgres for the full kit-and-kaboodle.
  // This contains a lot of what we want, but it's a little hard to parse out,
  // so we ask in other ways too.
  let fullPgDump = (await pgDumpDb()).stdout;

  const schemaToCommandTuples = Object.fromEntries(
    _.flatten(
      await Promise.all(
        schemaNames.map(async (schemaName) => {
          const tableNames = await getTableNamesFromPostgres(schemaName);
          const functionNames = await getFunctionNamesFromPostgres(schemaName);
          const viewNames = await getViewNamesFromPostgres(schemaName);

          // Then, we construct our sql commands to get the data we need,
          // based on what we know of the names in our schema.
          const tableEntries = tableNames.map((name, idx) => [
            `TABLE.DUMP:${schemaName}.${name}`,
            dumpTableBySchemaAndNameCommandString(schemaName, name),
          ]);
          const tableDescribeEntries = tableNames.map((name, idx) => [
            `TABLE.DESCRIBE:${schemaName}.${name}`,
            getTableDescriptionCommandString(schemaName, name),
          ]);
          const functionEntries = functionNames.map((name, idx) => [
            `FUNCTION.DUMP:${schemaName}.${name}`,
            dumpFunctionBySchemaAndNameCommandString(schemaName, name),
          ]);
          const viewEntries = viewNames.map((name, idx) => [
            `VIEW.DUMP:${schemaName}.${name}`,
            dumpViewBySchemaAndNameCommandString(schemaName, name),
          ]);

          return [
            ...tableEntries,
            ...tableDescribeEntries,
            ...functionEntries,
            ...viewEntries,
          ];
        })
      )
    )
  );

  try {
    const resultObj = await runMultipleDatabaseCommandsObj(
      schemaToCommandTuples
    );

    const tableKeys = Object.keys(resultObj).filter((k) =>
      startsWith(k, "TABLE.DUMP")
    );
    const tableDescribeKeys = Object.keys(resultObj).filter((k) =>
      startsWith(k, "TABLE.DESCRIBE")
    );
    const functionKeys = Object.keys(resultObj).filter((k) =>
      startsWith(k, "FUNCTION.DUMP")
    );
    const viewKeys = Object.keys(resultObj).filter((k) =>
      startsWith(k, "VIEW.DUMP")
    );

    // Write out tables.
    tableKeys.forEach(async (k, idx) => {
      const [operationType, fullName] = k.split(":");
      const [schemaName, entityName] = fullName.split(".");

      try {
        // 1. Create the table's entry in the tables list.
        const tableDump = resultObj[k]?.stdout;
        const description =
          resultObj[`TABLE.DESCRIBE:${schemaName}.${entityName}`]?.stdout;

        if (
          tableDump === undefined ||
          description === undefined ||
          tableDump.trim().length < 1 ||
          description.trim().length < 1
        ) {
          throw Error(`Did not receive results for ${fullName}.`);
        }

        // Prepend Intro message
        const writing =
          WARN_AUTOGEN_SQL +
          "\n" +
          `-- Table: ${fullName}\n` +
          sectionWrapContents(
            `TABLE DESCRIPTION (TABLE: ${fullName})`,
            prefixLines(description, "-- ") + "\n"
          ) +
          "\n" +
          sectionWrapContents(
            `PG_DUMP RESULT (TABLE: ${fullName})`,
            tableDump + "\n"
          ) +
          "\n";

        RET[schemaName].meta.numTables++;
        RET[schemaName].entities.push({
          schemaName,
          entityName,
          entityType: "table",
          dataToWrite: writing,
        });
      } catch (err: any) {
        console.error(`Issue writing ${fullName}. Skipping.`);
        console.error(`ERR: ${err}`);
      }
    });

    // FUNCTIONS
    // Write out functions.
    functionKeys.map(async (k, idx) => {
      const [operationType, fullName] = k.split(":");
      const [schemaName, entityName] = fullName.split(".");

      try {
        const basicPgDump = resultObj[k]?.stdout;
        if (isEmpty(basicPgDump) || isWhitespace(basicPgDump)) {
          throw Error(`Did not receive results for ${fullName}.`);
        }

        // GRANT lines that include the function name.
        // We do this to make up for the fact that the default exporter doesn't export permissions on functions.
        const extendedPgDump = fullPgDump
          .split("\n")
          .filter((s) => s.includes(fullName))
          .filter((s) => s.includes("GRANT"))
          .join("\n");

        const writing =
          WARN_AUTOGEN_SQL +
          "\n" +
          `-- Function: ${fullName}\n` +
          sectionWrapContents(
            `PG_DUMP RESULT (FUNCTION: ${fullName})`,
            `${basicPgDump}\n${extendedPgDump}\n`
          );

        RET[schemaName].meta.numFunctions++;
        RET[schemaName].entities.push({
          schemaName,
          entityName,
          entityType: "function",
          dataToWrite: writing,
        });
      } catch (err) {
        console.error(`Issue writing ${fullName}. Skipping. (ERR: ${err})`);
      }
    });

    // VIEWS
    viewKeys.map(async (k, idx) => {
      const [operationType, fullName] = k.split(":");
      const [schemaName, entityName] = fullName.split(".");
      try {
        const result = resultObj[k]?.stdout;

        if (isEmpty(result) || isWhitespace(result)) {
          throw Error(`Did not receive results for ${fullName}.`);
        }
        const writing =
          WARN_AUTOGEN_SQL +
          "\n" +
          `-- View: ${fullName}\n` +
          sectionWrapContents(`PG_DUMP RESULT (VIEW: ${fullName})`, result);

        RET[schemaName].meta.numViews++;
        RET[schemaName].entities.push({
          schemaName,
          entityName,
          entityType: "view",
          dataToWrite: writing,
        });
      } catch {
        console.error(`Issue writing ${fullName}. Skipping.`);
      }
    });
  } catch (err: any) {
    console.error(`oh no: ${err}`);
  }

  return RET;
}

async function getSchemaNames() {
  const command = `docker exec supabase_db_reasonote psql -h localhost -p 5432 -U postgres -d postgres -c '\\dn'`;
  const result = await util.promisify(exec)(command);

  const lines = result.stdout.split("\n");

  const schema_names = _.flatten(
    lines.slice(3).map((line) => {
      const schema_name_re = /^([^\|]+)\|[^\|]+/;

      const results = schema_name_re.exec(line);
      const group = results ? _.trim(results[1]) : null;

      return group;
    })
  );

  return schema_names.filter(notEmpty);
}

export async function supabaseReset() {
  const command = `supabase db reset`;
  return await util.promisify(exec)(command);
}

async function supabaseStart() {
  const command = `supabase start`;
  const supabaseLocationString = _.replace(
    (await util.promisify(exec)("which supabase")).stdout,
    "\n",
    ""
  );
  const supabaseVersionString = _.replace(
    (await util.promisify(exec)("supabase --version")).stdout,
    "\n",
    ""
  );

  console.log(
    `Using Supabase ${supabaseVersionString} (installed at: ${supabaseLocationString})`
  );
  return await util.promisify(exec)(command);
}

interface RegenSchemaCommandParams {
  selectedSchemas?: string[];
  resetDatabaseFirst?: boolean;
}

export class RegenSchemaCommand {
  static runCommand = async (params: RegenSchemaCommandParams) => {
    const { selectedSchemas } = params;
    console.log("Begin schema export!");

    console.log("🏁 🔄 Starting supabase...");

    try {
      await logTime("start-supabase", async () => {
        const sbStartOutput = (await supabaseStart()).stdout;
        console.log(`${sbStartOutput}`);
        console.log("🏁 ✅ Supabase started.\n");
      });
    } catch (err: any) {
      if (err.stderr.includes("supabase start is already running")) {
        console.log("🏁 ✅ Supabase already running.\n");
      } else {
        throw err;
      }
    }

    if (params.resetDatabaseFirst) {
      // Clearing Supabase Data.
      console.log(
        "🆕 🔄 Clearing Supabase data & Re-running supabase migrations..."
      );
      const sbRet = await logTime("supabase-reset", async () => {
        return (await supabaseReset());
      });

      console.log(prefixAllLines("stdout: ", sbRet.stdout));
      console.log(prefixAllLines("stderr: ", sbRet.stderr));
      console.log("🆕 ✅ Supabase database reset.\n");
    } else {
      console.warn(
        "⚠️Not re-migrating from scratch!\n⚠️Recall that the most accurate version of the schema comes by re-migrating from scratch.\n"
      );
    }

    // Get Schema names.
    // Filtering to only selected Schemas, if desired.
    console.log("ℹ️  🔄 Accessing database to get schema names...");
    const schemaNames = await logTime("get-schema-names", async () => {
      return (await getSchemaNames()).filter((s) =>
        selectedSchemas ? selectedSchemas.includes(s) : s
      );
    });
    console.log(`ℹ️  ✅ Got schema names: ${schemaNames}\n`);

    // Get contents of all files to write.
    console.log("📃 🔄 Accessing database to get new schema file contents...");
    const regenSchemaContents = await logTime(
      "get-new-schema-contents",
      async () => {
        return await getRegenSchemas(schemaNames);
      }
    );

    // const pathToContents = contentsPerSchema.reduce((prev, cur, idx) => ({...prev, ...cur}))
    console.log("📃 ✅ Got new schema file contents!\n");

    // Clear out existing schema folders, for the schemas we have chosen.
    console.log(
      `🗑  🔄 Clearing out selected schemas (${schemaNames}) under '${SUPABASE_SCHEMA_DIR_PATH}'...`
    );
    await logTime("remove-old-schema-directories", async () => {
      return schemaNames?.map((schemaName) =>
        rmSync(join(SUPABASE_SCHEMA_DIR_PATH, schemaName), {
          recursive: true,
          force: true,
        })
      );
    });
    console.log("🗑  ✅  Cleared out existing schema!\n");

    // Write contents to corresponding files.
    console.log(
      `📝 🔄 Filling in new schema contents under: '${SUPABASE_SCHEMA_DIR_PATH}'...`
    );
    let totalNumTables = 0,
      totalNumViews = 0,
      totalNumFunctions = 0;
    const res = await Promise.all(
      _.entries(regenSchemaContents).map(async ([schemaName, schemaData]) => {
        schemaData.entities.map(async (entity) => {
          // TODO use pluralizer instead.
          const entityTypeFolderName = `${entity.entityType}s`;
          const path = `${SUPABASE_SCHEMA_DIR_PATH}/${schemaName}/${entityTypeFolderName}/${entity.entityName}.sql`;

          if (entity.entityType === "function") totalNumFunctions++;
          if (entity.entityType === "view") totalNumViews++;
          if (entity.entityType === "table") totalNumTables++;

          await writeFileRecursiveCreate(path, entity.dataToWrite);
        });
      })
    );
    _.entries(regenSchemaContents).forEach(([schemaName, v]) => {
      const { numFunctions, numTables, numViews } = v.meta;
      console.log(
        `| Schema ${schemaName}: ${_.repeat(
          " ",
          20 - schemaName.length
        )}Wrote ${numFunctions} Functions, ${numViews} Views, and ${numTables} Tables Successfully.`
      );
    });
    console.log("|");
    console.log(
      `| TOTAL: Wrote ${totalNumFunctions} Functions, ${totalNumViews} Views, and ${totalNumTables} Tables Successfully.`
    );
    console.log(`📝 ✅ Schema contents written!\n`);

    return;
  };
}

async function regenSchemaCommand(params: RegenSchemaCommandParams) { }

///////////////////////////////////////////////////////////////////////////////////////////
// RUN MAIN
/////////////////////

if (require.main === module) {
  const program = new Command();
  program
    .name("supabase:db:regen-schema")
    .description("CLI to regenerate supabase schema.")
    .version("0.0.0")
    .option(
      "-r --resetDatabaseFirst",
      "DANGEROUS: Delete your local public schema (if it exists) and regenerate it from your migrations. THIS WILL DELETE ALL YOUR LOCAL DATA & SCHEMAS."
    )
    .option(
      "-s --selectedSchemas <selectedSchemas>",
      "comma separated, space-less schemas (i.e. 'public,auth')",
      "public"
    );

  program.parse(process.argv);

  const { selectedSchemas, resetDatabaseFirst } = program.opts();

  RegenSchemaCommand.runCommand({
    selectedSchemas: selectedSchemas ? selectedSchemas.split(",") : undefined,
    resetDatabaseFirst,
  })
    .then(() => {
      console.log("✅ All done!");
    })
    .catch((err) => {
      const errStr = err.toString();

      const errStrLines = errStr.split("\n");

      console.error(trimAllLines(`
      ❌ There were issues while running this job.
      ❌ ERROR RETURNED:
      ${errStrLines.map((line: string) => `❌ | ${line}`).join("\n")}
      `));

      // TODO: ugly
      process.exit(1);
    });
}
